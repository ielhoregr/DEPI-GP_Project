{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2233682,"sourceType":"datasetVersion","datasetId":1340369}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Libraries ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np \n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:31:40.353083Z","iopub.execute_input":"2024-10-17T08:31:40.353362Z","iopub.status.idle":"2024-10-17T08:31:41.646844Z","shell.execute_reply.started":"2024-10-17T08:31:40.353330Z","shell.execute_reply":"2024-10-17T08:31:41.645801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### load data","metadata":{}},{"cell_type":"code","source":"\ntrain_df = pd.read_csv('/kaggle/input/amazon-reviews/train.csv',header=None, names=['polarity', 'title', 'review'])\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:31:41.648727Z","iopub.execute_input":"2024-10-17T08:31:41.649161Z","iopub.status.idle":"2024-10-17T08:32:17.985565Z","shell.execute_reply.started":"2024-10-17T08:31:41.649126Z","shell.execute_reply":"2024-10-17T08:32:17.984165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.sample(5)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:32:17.986934Z","iopub.execute_input":"2024-10-17T08:32:17.987301Z","iopub.status.idle":"2024-10-17T08:32:18.120461Z","shell.execute_reply.started":"2024-10-17T08:32:17.987268Z","shell.execute_reply":"2024-10-17T08:32:18.119384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### check class imbalance","metadata":{}},{"cell_type":"code","source":"train_df.polarity.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:32:18.123266Z","iopub.execute_input":"2024-10-17T08:32:18.123625Z","iopub.status.idle":"2024-10-17T08:32:18.172115Z","shell.execute_reply.started":"2024-10-17T08:32:18.123589Z","shell.execute_reply":"2024-10-17T08:32:18.171025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# map polarity : positive ->0 | negative -> 1\ntrain_df['label'] = train_df.polarity.map({2:0, 1:1})\ntrain_df.sample(3)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:32:18.173295Z","iopub.execute_input":"2024-10-17T08:32:18.173605Z","iopub.status.idle":"2024-10-17T08:32:18.290792Z","shell.execute_reply.started":"2024-10-17T08:32:18.173575Z","shell.execute_reply":"2024-10-17T08:32:18.289858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No class imbalance","metadata":{}},{"cell_type":"code","source":"# check missing value in review column \nlen(train_df[train_df.review.isnull()])","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:32:18.292434Z","iopub.execute_input":"2024-10-17T08:32:18.293046Z","iopub.status.idle":"2024-10-17T08:32:18.770510Z","shell.execute_reply.started":"2024-10-17T08:32:18.292990Z","shell.execute_reply":"2024-10-17T08:32:18.769564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.review.isnull().any()","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:32:18.771791Z","iopub.execute_input":"2024-10-17T08:32:18.772249Z","iopub.status.idle":"2024-10-17T08:32:19.253720Z","shell.execute_reply.started":"2024-10-17T08:32:18.772214Z","shell.execute_reply":"2024-10-17T08:32:19.252699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check missing value in title column \nlen(train_df[train_df.title.isnull()])","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:32:19.255062Z","iopub.execute_input":"2024-10-17T08:32:19.255727Z","iopub.status.idle":"2024-10-17T08:32:19.601396Z","shell.execute_reply.started":"2024-10-17T08:32:19.255683Z","shell.execute_reply":"2024-10-17T08:32:19.600330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"a 207 NaN value in title, this will create a problem if we planning to combine the title nad review in a single row ","metadata":{}},{"cell_type":"code","source":"# combine review title with review body \n# fillna : replaces any NaN values in the title  an empty string \n\ntrain_df['full_review'] = train_df.title.fillna('') + \" \" + train_df.review ","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:32:19.602710Z","iopub.execute_input":"2024-10-17T08:32:19.603130Z","iopub.status.idle":"2024-10-17T08:32:22.350499Z","shell.execute_reply.started":"2024-10-17T08:32:19.603095Z","shell.execute_reply":"2024-10-17T08:32:22.349470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-16T17:13:20.278230Z","iopub.execute_input":"2024-10-16T17:13:20.278567Z","iopub.status.idle":"2024-10-16T17:13:20.289577Z","shell.execute_reply.started":"2024-10-16T17:13:20.278533Z","shell.execute_reply":"2024-10-16T17:13:20.288597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to check that the new 'full_review' is not affected by the Nan values in title\ntrain_df[train_df.title.isnull()]","metadata":{"execution":{"iopub.status.busy":"2024-10-16T15:25:35.150317Z","iopub.execute_input":"2024-10-16T15:25:35.150652Z","iopub.status.idle":"2024-10-16T15:25:35.525676Z","shell.execute_reply.started":"2024-10-16T15:25:35.150619Z","shell.execute_reply":"2024-10-16T15:25:35.524581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.title[0]","metadata":{"execution":{"iopub.status.busy":"2024-10-16T15:25:35.526917Z","iopub.execute_input":"2024-10-16T15:25:35.527287Z","iopub.status.idle":"2024-10-16T15:25:35.534762Z","shell.execute_reply.started":"2024-10-16T15:25:35.527248Z","shell.execute_reply":"2024-10-16T15:25:35.533524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.review[0]","metadata":{"execution":{"iopub.status.busy":"2024-10-16T15:25:35.536349Z","iopub.execute_input":"2024-10-16T15:25:35.536892Z","iopub.status.idle":"2024-10-16T15:25:35.544367Z","shell.execute_reply.started":"2024-10-16T15:25:35.536842Z","shell.execute_reply":"2024-10-16T15:25:35.543383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.full_review[0]","metadata":{"execution":{"iopub.status.busy":"2024-10-16T15:25:35.545584Z","iopub.execute_input":"2024-10-16T15:25:35.545897Z","iopub.status.idle":"2024-10-16T15:25:35.556653Z","shell.execute_reply.started":"2024-10-16T15:25:35.545866Z","shell.execute_reply":"2024-10-16T15:25:35.555689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check dublicate \n\nduplicate_rows = train_df.duplicated() \nduplicate_rows.any() ","metadata":{"execution":{"iopub.status.busy":"2024-10-16T15:25:35.557942Z","iopub.execute_input":"2024-10-16T15:25:35.559046Z","iopub.status.idle":"2024-10-16T15:25:55.021908Z","shell.execute_reply.started":"2024-10-16T15:25:35.559008Z","shell.execute_reply":"2024-10-16T15:25:55.020920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No Dublicates in the data ","metadata":{}},{"cell_type":"code","source":"len(train_df.full_review[0])","metadata":{"execution":{"iopub.status.busy":"2024-10-16T15:25:55.023146Z","iopub.execute_input":"2024-10-16T15:25:55.023450Z","iopub.status.idle":"2024-10-16T15:25:55.031287Z","shell.execute_reply.started":"2024-10-16T15:25:55.023418Z","shell.execute_reply":"2024-10-16T15:25:55.030225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_df.full_review[100])","metadata":{"execution":{"iopub.status.busy":"2024-10-16T15:25:55.032679Z","iopub.execute_input":"2024-10-16T15:25:55.032988Z","iopub.status.idle":"2024-10-16T15:25:55.042894Z","shell.execute_reply.started":"2024-10-16T15:25:55.032956Z","shell.execute_reply":"2024-10-16T15:25:55.042044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.full_review[100]","metadata":{"execution":{"iopub.status.busy":"2024-10-16T15:25:55.044217Z","iopub.execute_input":"2024-10-16T15:25:55.044883Z","iopub.status.idle":"2024-10-16T15:25:55.053233Z","shell.execute_reply.started":"2024-10-16T15:25:55.044839Z","shell.execute_reply":"2024-10-16T15:25:55.052287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_df.full_review[1])","metadata":{"execution":{"iopub.status.busy":"2024-10-16T15:25:55.054398Z","iopub.execute_input":"2024-10-16T15:25:55.054642Z","iopub.status.idle":"2024-10-16T15:25:55.063393Z","shell.execute_reply.started":"2024-10-16T15:25:55.054615Z","shell.execute_reply":"2024-10-16T15:25:55.062540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check avergae lenght of a full review \nfull_review_avg_length = np.mean([ len(train_df.full_review[0]) for i in range(len(train_df.full_review))])\nprint(full_review_avg_length) ","metadata":{"execution":{"iopub.status.busy":"2024-10-16T15:25:55.064511Z","iopub.execute_input":"2024-10-16T15:25:55.064789Z","iopub.status.idle":"2024-10-16T15:26:50.743967Z","shell.execute_reply.started":"2024-10-16T15:25:55.064759Z","shell.execute_reply":"2024-10-16T15:26:50.742944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check for very short reviews \n\nshort_reviews = train_df[train_df['full_review'].apply(lambda x: len(x.split()) < 10)]\n\nshort_reviews.label.value_counts()\n\nprint(f\"Number of short reviews: {len(short_reviews)}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-16T15:26:50.745360Z","iopub.execute_input":"2024-10-16T15:26:50.745700Z","iopub.status.idle":"2024-10-16T15:27:10.333970Z","shell.execute_reply.started":"2024-10-16T15:26:50.745663Z","shell.execute_reply":"2024-10-16T15:27:10.333016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"short_reviews.head() ","metadata":{"execution":{"iopub.status.busy":"2024-10-16T15:27:10.335260Z","iopub.execute_input":"2024-10-16T15:27:10.336244Z","iopub.status.idle":"2024-10-16T15:27:10.347430Z","shell.execute_reply.started":"2024-10-16T15:27:10.336202Z","shell.execute_reply":"2024-10-16T15:27:10.345784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"short_reviews.full_review.iloc[0]","metadata":{"execution":{"iopub.status.busy":"2024-10-16T15:27:10.348611Z","iopub.execute_input":"2024-10-16T15:27:10.348885Z","iopub.status.idle":"2024-10-16T15:27:10.368255Z","shell.execute_reply.started":"2024-10-16T15:27:10.348854Z","shell.execute_reply":"2024-10-16T15:27:10.367298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a lot of garbage and noise that need our attention ","metadata":{}},{"cell_type":"markdown","source":"### a random sample of train_df for ease of experiments","metadata":{}},{"cell_type":"code","source":"# a lighter df to experiment with \ndf_train_sample  = train_df.sample(100000, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T15:27:10.369557Z","iopub.execute_input":"2024-10-16T15:27:10.370365Z","iopub.status.idle":"2024-10-16T15:27:10.537046Z","shell.execute_reply.started":"2024-10-16T15:27:10.370327Z","shell.execute_reply":"2024-10-16T15:27:10.536132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_sample.label.value_counts() ","metadata":{"execution":{"iopub.status.busy":"2024-10-16T15:27:10.538290Z","iopub.execute_input":"2024-10-16T15:27:10.538616Z","iopub.status.idle":"2024-10-16T15:27:10.548734Z","shell.execute_reply.started":"2024-10-16T15:27:10.538582Z","shell.execute_reply":"2024-10-16T15:27:10.547785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df_train_sample)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T15:27:10.550028Z","iopub.execute_input":"2024-10-16T15:27:10.550434Z","iopub.status.idle":"2024-10-16T15:27:10.558587Z","shell.execute_reply.started":"2024-10-16T15:27:10.550396Z","shell.execute_reply":"2024-10-16T15:27:10.557560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Remove URL and HTML tags if any","metadata":{}},{"cell_type":"code","source":"from bs4 import BeautifulSoup\nhas_html_tags = df_train_sample['full_review'].apply(lambda x: bool(BeautifulSoup(x, \"html.parser\").find(True)))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T15:27:10.559579Z","iopub.execute_input":"2024-10-16T15:27:10.559819Z","iopub.status.idle":"2024-10-16T15:27:16.787937Z","shell.execute_reply.started":"2024-10-16T15:27:10.559792Z","shell.execute_reply":"2024-10-16T15:27:16.787042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_sample[has_html_tags].full_review.iloc[1]","metadata":{"execution":{"iopub.status.busy":"2024-10-16T15:27:16.792468Z","iopub.execute_input":"2024-10-16T15:27:16.792915Z","iopub.status.idle":"2024-10-16T15:27:16.800814Z","shell.execute_reply.started":"2024-10-16T15:27:16.792881Z","shell.execute_reply":"2024-10-16T15:27:16.799948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"there is indeed html tags that will require our attention ","metadata":{}},{"cell_type":"code","source":"# check for urls \nimport re\nhas_url = [bool(re.findall(r'http\\S+', df_train_sample.full_review.iloc[i])) for i in range(len(df_train_sample))]\ndf_train_sample[has_url]","metadata":{"execution":{"iopub.status.busy":"2024-10-16T15:27:16.801752Z","iopub.execute_input":"2024-10-16T15:27:16.802049Z","iopub.status.idle":"2024-10-16T15:27:19.140034Z","shell.execute_reply.started":"2024-10-16T15:27:16.802018Z","shell.execute_reply":"2024-10-16T15:27:19.139114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"both URL and HTML tags do not affect the feeling of the customer, hence they are just noise in our case.","metadata":{}},{"cell_type":"markdown","source":"### Preprocessing Steps:\n\n1. **Remove URLs**: Strips any URLs from the review, as they don't contribute meaningfully to sentiment analysis.\n\n2. **Remove Gibberish & Excessive Repeated Characters**: Identifies and removes sequences with too many repeated characters (e.g., \"ooooo\" becomes \"o\") and repeated words (e.g., \"great great\" becomes \"great\").\n\n3. **Remove Punctuation, Numbers, and Long Words**: Filters out punctuation, numbers, and excessively long words (over 20 characters).\n\n4. **Lowercase Text**: Converts all text to lowercase.\n\n5. **Remove Stopwords (except \"not\", \"no\", and \"nor\")**: Removes common stopwords except negations to preserve their importance.\n\n6. **Lemmatization**: Converts words to their base form (e.g., \"running\" to \"run\").\n\n7. **Word Embeding*\n","metadata":{}},{"cell_type":"code","source":"import re\nimport spacy\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import FunctionTransformer, MinMaxScaler, StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom tqdm import tqdm\nimport pandas as pd\nfrom joblib import Parallel, delayed\n\n\n\nnlp = spacy.load('en_core_web_lg')\n\n# Custom stopwords list to keep \"not\", \"no\", \"nor\"\nstopwords_to_keep = {\"not\", \"no\", \"nor\"}\ncustom_stopwords = nlp.Defaults.stop_words - stopwords_to_keep\n\n# Pre-compile regex patterns for optimization\nurl_pattern = re.compile(r'http\\S+|www\\S+|https\\S+')\nrepeated_char_pattern = re.compile(r'(.)\\1{3,}')\nrepeated_word_pattern = re.compile(r'\\b(\\w+)( \\1\\b)+')\n\ndef remove_gibberish(text):\n    # Remove excessive repeated characters \n    text = repeated_char_pattern.sub(r'\\1', text)\n    # Remove repeated words \n    text = repeated_word_pattern.sub(r'\\1', text)\n    return text\n\ndef preprocess_text(text):\n    text = url_pattern.sub('', text)\n    text = remove_gibberish(text)\n    \n    doc = nlp(text)\n    \n    tokens = [token.lemma_.lower() for token in doc if not (\n        token.is_punct or token.is_digit or len(token.text) > 20 or token.text in custom_stopwords)]\n    \n    clean_text = \" \".join(tokens)\n    \n    # return review vector representation  \n    return nlp(clean_text).vector\n\n\ndef combined_preprocessing(text_series):\n    # Use parallel processing with joblib to preprocess text in parallel\n    results = Parallel(n_jobs=-1, backend=\"multiprocessing\")(\n        delayed(preprocess_text)(text) for text in tqdm(text_series, desc=\"Processing texts\")\n    )\n    return pd.Series(results)\n\n\n# Function to stack the preprocessed vectors\ndef stack_preprocessed(preprocessed_series):\n    return np.stack(preprocessed_series)\n\n\n# Custom transformer for stacking\nclass StackingTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        return stack_preprocessed(X)\n\n\npreprocess_pipe = Pipeline([\n    ('preprocess', FunctionTransformer(combined_preprocessing, validate=False)),  # Preprocess the text\n    ('stacking', StackingTransformer()),\n    ('scaling', MinMaxScaler())\n])\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T15:27:19.141652Z","iopub.execute_input":"2024-10-16T15:27:19.142054Z","iopub.status.idle":"2024-10-16T15:27:31.487175Z","shell.execute_reply.started":"2024-10-16T15:27:19.142010Z","shell.execute_reply":"2024-10-16T15:27:31.486351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train/Test Split (Using Subset Of Data)","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df_sample  = train_df.sample(1000, random_state = 42)\n\nX_train,X_test, y_train, y_test = train_test_split(\n    train_df_sample.full_review,\n    train_df_sample.label,\n    random_state=42\n)\n\nX_train.shape, X_test.shape\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T15:27:38.715139Z","iopub.execute_input":"2024-10-16T15:27:38.716289Z","iopub.status.idle":"2024-10-16T15:27:38.910558Z","shell.execute_reply.started":"2024-10-16T15:27:38.716246Z","shell.execute_reply":"2024-10-16T15:27:38.909401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test preprocess pipe \nX_train_pre = preprocess_pipe.fit_transform(X_train)\nX_test_pre = preprocess_pipe.fit_transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T15:27:58.868647Z","iopub.execute_input":"2024-10-16T15:27:58.869173Z","iopub.status.idle":"2024-10-16T15:28:15.392349Z","shell.execute_reply.started":"2024-10-16T15:27:58.869114Z","shell.execute_reply":"2024-10-16T15:28:15.391103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_pre","metadata":{"execution":{"iopub.status.busy":"2024-10-16T15:28:16.872169Z","iopub.execute_input":"2024-10-16T15:28:16.873228Z","iopub.status.idle":"2024-10-16T15:28:16.881093Z","shell.execute_reply.started":"2024-10-16T15:28:16.873178Z","shell.execute_reply":"2024-10-16T15:28:16.880144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test that every thing is working","metadata":{"execution":{"iopub.status.busy":"2024-10-14T07:11:11.957427Z","iopub.execute_input":"2024-10-14T07:11:11.957913Z","iopub.status.idle":"2024-10-14T07:11:11.964977Z","shell.execute_reply.started":"2024-10-14T07:11:11.957868Z","shell.execute_reply":"2024-10-14T07:11:11.963086Z"}}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlog_reg = make_pipeline(preprocess_pipe, LogisticRegression())\n\nlog_reg.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T15:24:32.820550Z","iopub.execute_input":"2024-10-15T15:24:32.820953Z","iopub.status.idle":"2024-10-15T15:24:42.831147Z","shell.execute_reply.started":"2024-10-15T15:24:32.820912Z","shell.execute_reply":"2024-10-15T15:24:42.829784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, log_reg.predict(X_test))) ","metadata":{"execution":{"iopub.status.busy":"2024-10-15T15:24:42.833316Z","iopub.execute_input":"2024-10-15T15:24:42.834154Z","iopub.status.idle":"2024-10-15T15:24:46.624450Z","shell.execute_reply.started":"2024-10-15T15:24:42.834102Z","shell.execute_reply":"2024-10-15T15:24:46.623035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling","metadata":{}},{"cell_type":"markdown","source":"- Working with only a random subset of the data.\n- The preprocessing step is not in the pipeline to save some time. ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df_sample  = train_df.sample(200000, random_state = 42)\n\nX_train,X_test, y_train, y_test = train_test_split(\n    train_df_sample.full_review,\n    train_df_sample.label,\n    test_size=.2,\n    random_state=42\n)\n\nX_train.shape, X_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-15T15:24:46.631048Z","iopub.execute_input":"2024-10-15T15:24:46.635492Z","iopub.status.idle":"2024-10-15T15:24:47.566176Z","shell.execute_reply.started":"2024-10-15T15:24:46.635412Z","shell.execute_reply":"2024-10-15T15:24:47.564684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To test more models, we will do the preprocessing in a seperate step to save us some time \n\n# Import necessary libraries\nimport pandas as pd\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nimport xgboost as xgb\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tqdm import tqdm\n\n\n\nX_train_pre = preprocess_pipe.fit_transform(X_train)\nX_test_pre = preprocess_pipe.fit_transform(X_test)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T15:24:47.567620Z","iopub.execute_input":"2024-10-15T15:24:47.568028Z","iopub.status.idle":"2024-10-15T16:02:48.150202Z","shell.execute_reply.started":"2024-10-15T15:24:47.567989Z","shell.execute_reply":"2024-10-15T16:02:48.148313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###### Dictionary of models\nmodels = {\n    \"Logistic Regression\": LogisticRegression(solver='lbfgs', max_iter=1000),\n    \"SVM\": SVC(),\n    \"Random Forest\": RandomForestClassifier(),\n    \"XGBoost\": xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n    \"MLP Neural Network\": MLPClassifier(max_iter=1000)\n}\n\n\n\ndef evaluate_model(model_name, model, X_train_pre, y_train):\n    scores = cross_val_score(model, X_train_pre, y_train, cv=5, scoring='f1_macro', n_jobs=-1) \n    return model_name, scores.mean()\n\nresults = []\nfor model_name, model in tq dm(models.items(), desc=\"Evaluating models\"):\n    model_name, score = evaluate_model(model_name, model, X_train_pre, y_train)\n    results.append((model_name, score))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T16:02:48.153390Z","iopub.execute_input":"2024-10-15T16:02:48.153863Z","iopub.status.idle":"2024-10-15T19:07:13.845609Z","shell.execute_reply.started":"2024-10-15T16:02:48.153814Z","shell.execute_reply":"2024-10-15T19:07:13.843323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_df = pd.DataFrame(results, columns=['Model', 'Mean Accuracy'])\nresults_df\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T19:07:22.049966Z","iopub.execute_input":"2024-10-15T19:07:22.051102Z","iopub.status.idle":"2024-10-15T19:07:22.062862Z","shell.execute_reply.started":"2024-10-15T19:07:22.051053Z","shell.execute_reply":"2024-10-15T19:07:22.061593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nbest_model_name = results_df.loc[results_df['Mean Accuracy'].idxmax(), 'Model']\nbest_score = results_df['Mean Accuracy'].max()\n\nbest_model = models[best_model_name]\n\nprint(f\"Best model: {best_model_name} with Mean Accuracy: {best_score}\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T19:11:06.404228Z","iopub.execute_input":"2024-10-15T19:11:06.404784Z","iopub.status.idle":"2024-10-15T19:11:06.417768Z","shell.execute_reply.started":"2024-10-15T19:11:06.404719Z","shell.execute_reply":"2024-10-15T19:11:06.416416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model.fit(X_train_pre, y_train) ","metadata":{"execution":{"iopub.status.busy":"2024-10-15T20:39:35.841928Z","iopub.execute_input":"2024-10-15T20:39:35.843367Z","iopub.status.idle":"2024-10-15T21:49:00.153086Z","shell.execute_reply.started":"2024-10-15T20:39:35.843313Z","shell.execute_reply":"2024-10-15T21:49:00.150876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the best model using joblib\njoblib.dump(best_model, f'best_model_{best_model_name}.pkl')\n\nprint(f\"The best model {best_model_name} has been saved as 'best_model_{best_model_name}.pkl'.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-15T21:50:53.399254Z","iopub.execute_input":"2024-10-15T21:50:53.400013Z","iopub.status.idle":"2024-10-15T21:50:53.950128Z","shell.execute_reply.started":"2024-10-15T21:50:53.399943Z","shell.execute_reply":"2024-10-15T21:50:53.948967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nimport joblib\n\n\ny_pred = best_model.predict(X_test_pre)\n\n# Evaluate the performance\naccuracy = accuracy_score(y_test, y_pred)\nreport = classification_report(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nprint(f\"Accuracy on test set: {accuracy}\")\nprint(\"Classification Report:\")\nprint(report)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T21:51:19.947425Z","iopub.execute_input":"2024-10-15T21:51:19.947903Z","iopub.status.idle":"2024-10-15T22:07:09.620227Z","shell.execute_reply.started":"2024-10-15T21:51:19.947860Z","shell.execute_reply":"2024-10-15T22:07:09.618561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## BERT Model Fine-Tuning","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nimport datetime\nimport gc\nimport random","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:33:08.836988Z","iopub.execute_input":"2024-10-17T08:33:08.837372Z","iopub.status.idle":"2024-10-17T08:33:08.841988Z","shell.execute_reply.started":"2024-10-17T08:33:08.837337Z","shell.execute_reply":"2024-10-17T08:33:08.840990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler,random_split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nimport transformers\nfrom transformers import BertForSequenceClassification, AdamW, BertConfig,BertTokenizer,get_linear_schedule_with_warmup","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:33:08.843696Z","iopub.execute_input":"2024-10-17T08:33:08.844365Z","iopub.status.idle":"2024-10-17T08:33:15.474248Z","shell.execute_reply.started":"2024-10-17T08:33:08.844320Z","shell.execute_reply":"2024-10-17T08:33:15.473398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### same preprocess as upove but without the word embeding ","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:33:15.475757Z","iopub.execute_input":"2024-10-17T08:33:15.476220Z","iopub.status.idle":"2024-10-17T08:33:15.535275Z","shell.execute_reply.started":"2024-10-17T08:33:15.476186Z","shell.execute_reply":"2024-10-17T08:33:15.534423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport spacy\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import FunctionTransformer, MinMaxScaler, StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom tqdm import tqdm\nimport pandas as pd\nfrom joblib import Parallel, delayed\n\n\n\nnlp = spacy.load('en_core_web_lg')\n\n# Custom stopwords list to keep \"not\", \"no\", \"nor\"\nstopwords_to_keep = {\"not\", \"no\", \"nor\"}\ncustom_stopwords = nlp.Defaults.stop_words - stopwords_to_keep\n\n# Pre-compile regex patterns for optimization\nurl_pattern = re.compile(r'http\\S+|www\\S+|https\\S+')\nrepeated_char_pattern = re.compile(r'(.)\\1{3,}')\nrepeated_word_pattern = re.compile(r'\\b(\\w+)( \\1\\b)+')\n\ndef remove_gibberish(text):\n    # Remove excessive repeated characters \n    text = repeated_char_pattern.sub(r'\\1', text)\n    # Remove repeated words \n    text = repeated_word_pattern.sub(r'\\1', text)\n    return text\n\ndef preprocess_text(text):\n    text = url_pattern.sub('', text)\n    text = remove_gibberish(text)\n    \n    doc = nlp(text)\n    \n    tokens = [token.lemma_.lower() for token in doc if not (\n        token.is_punct or token.is_digit or len(token.text) > 20 or token.text in custom_stopwords)]\n    \n    clean_text = \" \".join(tokens)\n    \n    # return review vector representation  \n    return clean_text\n\n\ndef combined_preprocessing(text_series):\n    # Use parallel processing with joblib to preprocess text in parallel\n    results = Parallel(n_jobs=-1, backend=\"multiprocessing\")(\n        delayed(preprocess_text)(text) for text in tqdm(text_series, desc=\"Processing texts\")\n    )\n    return pd.Series(results)\n\n\n# Function to stack the preprocessed vectors\ndef stack_preprocessed(preprocessed_series):\n    return np.stack(preprocessed_series)\n\n\n# Custom transformer for stacking\nclass StackingTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        return stack_preprocessed(X)\n\n\npreprocess_pipe = Pipeline([\n    ('preprocess', FunctionTransformer(combined_preprocessing, validate=False)),  # Preprocess the text\n    #('stacking', StackingTransformer()),\n])\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:33:15.536790Z","iopub.execute_input":"2024-10-17T08:33:15.537160Z","iopub.status.idle":"2024-10-17T08:33:23.933931Z","shell.execute_reply.started":"2024-10-17T08:33:15.537127Z","shell.execute_reply":"2024-10-17T08:33:23.933153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df_sample  = train_df.sample(150000, random_state = 42)\n\nX_train,X_test, y_train, y_test = train_test_split(\n    train_df_sample.full_review,\n    train_df_sample.label,\n    random_state=42,\n    test_size=.15,\n)\n\nX_train.shape, X_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:33:23.936365Z","iopub.execute_input":"2024-10-17T08:33:23.937346Z","iopub.status.idle":"2024-10-17T08:33:24.102376Z","shell.execute_reply.started":"2024-10-17T08:33:23.937289Z","shell.execute_reply":"2024-10-17T08:33:24.101430Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_pre = preprocess_pipe.fit_transform(X_train)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:33:24.103539Z","iopub.execute_input":"2024-10-17T08:33:24.103865Z","iopub.status.idle":"2024-10-17T08:50:41.624807Z","shell.execute_reply.started":"2024-10-17T08:33:24.103831Z","shell.execute_reply":"2024-10-17T08:50:41.623646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_pre = X_train_pre.values\nlabels = y_train.values ","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:50:41.626510Z","iopub.execute_input":"2024-10-17T08:50:41.626859Z","iopub.status.idle":"2024-10-17T08:50:41.631875Z","shell.execute_reply.started":"2024-10-17T08:50:41.626824Z","shell.execute_reply":"2024-10-17T08:50:41.630883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:50:41.633220Z","iopub.execute_input":"2024-10-17T08:50:41.633600Z","iopub.status.idle":"2024-10-17T08:50:42.494384Z","shell.execute_reply.started":"2024-10-17T08:50:41.633559Z","shell.execute_reply":"2024-10-17T08:50:42.493609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Original: {X_train_pre[0]}')\n\nprint(\"-\"*100)\n\n# The sentence split into tokens\nprint(f'Tokenized: {tokenizer.tokenize(X_train_pre[0])}')","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:50:42.495402Z","iopub.execute_input":"2024-10-17T08:50:42.495673Z","iopub.status.idle":"2024-10-17T08:50:42.501923Z","shell.execute_reply.started":"2024-10-17T08:50:42.495643Z","shell.execute_reply":"2024-10-17T08:50:42.500863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The sentence mapped to token ids \ntokens = tokenizer.tokenize(X_train_pre[0]) # for first instance \n\nprint(f'Token IDs : {tokenizer.convert_tokens_to_ids(tokens)}')","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:50:42.503151Z","iopub.execute_input":"2024-10-17T08:50:42.503485Z","iopub.status.idle":"2024-10-17T08:50:42.512827Z","shell.execute_reply.started":"2024-10-17T08:50:42.503453Z","shell.execute_reply":"2024-10-17T08:50:42.512007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len = 0 \n\nfor review in X_train_pre:\n    \n    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n    input_ids = tokenizer.encode(review, add_special_tokens=True)\n    \n    \n    max_len = max(max_len, len(input_ids))\n\n    \nprint(f'Max Sentence Length {max_len}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids = []\nattention_masks = [] \nmax_len = 200 \n\n# for every review \n\nfor review in X_train_pre:\n    \n    \n    encoded_dict = tokenizer.encode_plus(\n        review,                         # review to encode \n        add_special_tokens= True,       # add '[CLS]' and '[SEP]'\n        \n        max_length= max_len, \n        pad_to_max_length = True,      # pad & Truncate all sentences \n        return_attention_mask= True, \n        \n        return_tensors='pt'           # return a pytorch tensor \n    )\n    \n    input_ids.append(encoded_dict['input_ids'])\n    attention_masks.append(encoded_dict['attention_mask'])\n    \n\n    \n# convert the lists into tensors \n\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)\n\nlabels = torch.tensor(labels) ","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:52:01.837199Z","iopub.execute_input":"2024-10-17T08:52:01.837586Z","iopub.status.idle":"2024-10-17T08:56:16.072621Z","shell.execute_reply.started":"2024-10-17T08:52:01.837550Z","shell.execute_reply":"2024-10-17T08:56:16.071543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Original: ', X_train_pre[20])\n\nprint(\"-\"* 100)\nprint('Token IDs:', input_ids[20])","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:56:16.074757Z","iopub.execute_input":"2024-10-17T08:56:16.075253Z","iopub.status.idle":"2024-10-17T08:56:16.102931Z","shell.execute_reply.started":"2024-10-17T08:56:16.075207Z","shell.execute_reply":"2024-10-17T08:56:16.101947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = TensorDataset(input_ids, attention_masks, labels)   # every instance is a tuple of 3 \n\n\ntrain_size = int(len(dataset) * .8 )\nval_size = len(dataset) - train_size \n\n\ntrain_data, val_data = random_split(dataset, [train_size, val_size]) \n\nprint(f'{train_size} training samples') \nprint(f'{val_size} validation samples') \n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:56:16.104022Z","iopub.execute_input":"2024-10-17T08:56:16.104307Z","iopub.status.idle":"2024-10-17T08:56:16.126884Z","shell.execute_reply.started":"2024-10-17T08:56:16.104277Z","shell.execute_reply":"2024-10-17T08:56:16.126078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nbatch_size = 32 \n\n\n# train loader \ntrain_dataloader = DataLoader(\n    \n    train_data, \n    sampler= RandomSampler(train_data),   # select the batches randomly \n    batch_size = batch_size \n\n)\n\n\n# validation loader \nvalidation_dataloader = DataLoader(\n    \n    val_data, \n    sampler= RandomSampler(val_data),   # select the batches randomly \n    batch_size = batch_size \n\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:56:16.128749Z","iopub.execute_input":"2024-10-17T08:56:16.129062Z","iopub.status.idle":"2024-10-17T08:56:16.134150Z","shell.execute_reply.started":"2024-10-17T08:56:16.129030Z","shell.execute_reply":"2024-10-17T08:56:16.133225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load BertForSequenceClassification, the pretrained BERT model with a single \n\n# with linear classification layer on top \n\n\nmodel = BertForSequenceClassification.from_pretrained(\n    \n    \"bert-base-uncased\",\n    \n    num_labels = 2, \n    \n    output_attentions = False, \n    \n    output_hidden_states = False,\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:56:16.135329Z","iopub.execute_input":"2024-10-17T08:56:16.135642Z","iopub.status.idle":"2024-10-17T08:56:18.535699Z","shell.execute_reply.started":"2024-10-17T08:56:16.135612Z","shell.execute_reply":"2024-10-17T08:56:18.534979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:56:18.536773Z","iopub.execute_input":"2024-10-17T08:56:18.537088Z","iopub.status.idle":"2024-10-17T08:56:18.861819Z","shell.execute_reply.started":"2024-10-17T08:56:18.537054Z","shell.execute_reply":"2024-10-17T08:56:18.860814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(),\n                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n                )","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:56:18.863593Z","iopub.execute_input":"2024-10-17T08:56:18.864086Z","iopub.status.idle":"2024-10-17T08:56:19.411676Z","shell.execute_reply.started":"2024-10-17T08:56:18.864040Z","shell.execute_reply":"2024-10-17T08:56:19.410873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of training epochs. The BERT authors recommend between 2 and 4. \n# training data.\nepochs = 2\n\n# Total number of training steps is [number of batches] x [number of epochs]. \n# (Note that this is not the same as the number of training samples).\ntotal_steps = len(train_dataloader) * epochs\n\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:56:19.412675Z","iopub.execute_input":"2024-10-17T08:56:19.413194Z","iopub.status.idle":"2024-10-17T08:56:19.418320Z","shell.execute_reply.started":"2024-10-17T08:56:19.413158Z","shell.execute_reply":"2024-10-17T08:56:19.417465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:56:19.419421Z","iopub.execute_input":"2024-10-17T08:56:19.419733Z","iopub.status.idle":"2024-10-17T08:56:19.428727Z","shell.execute_reply.started":"2024-10-17T08:56:19.419701Z","shell.execute_reply":"2024-10-17T08:56:19.427865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:56:19.430985Z","iopub.execute_input":"2024-10-17T08:56:19.431309Z","iopub.status.idle":"2024-10-17T08:56:19.439872Z","shell.execute_reply.started":"2024-10-17T08:56:19.431272Z","shell.execute_reply":"2024-10-17T08:56:19.439117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nseed_val = 42\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\ntraining_stats = []\n\n# Measure the total training time for the whole run.\ntotal_t0 = time.time()\n\n# For each epoch...\nfor epoch_i in range(0, epochs):\n    \n\n    # Perform one full pass over the training set.\n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n    # Measure how long the training epoch takes.\n    t0 = time.time()\n    total_train_loss = 0\n    model.train()\n    for step, batch in enumerate(train_dataloader):\n        # Unpack this training batch from our dataloader. \n        #\n        # As we unpack the batch, we'll also copy each tensor to the device using the \n        # `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        optimizer.zero_grad()\n        output = model(b_input_ids, \n                             token_type_ids=None, \n                             attention_mask=b_input_mask, \n                             labels=b_labels)        \n        loss = output.loss\n        total_train_loss += loss.item()\n        # Perform a backward pass to calculate the gradients.\n        loss.backward()\n        # Clip the norm of the gradients to 1.0.\n        # This is to help prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        # Update parameters and take a step using the computed gradient.\n        # The optimizer dictates the \"update rule\"--how the parameters are\n        # modified based on their gradients, the learning rate, etc.\n        optimizer.step()\n        # Update the learning rate.\n        scheduler.step()\n\n    # Calculate the average loss over all of the batches.\n    avg_train_loss = total_train_loss / len(train_dataloader)            \n    \n    # Measure how long this epoch took.\n    training_time = format_time(time.time() - t0)\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epcoh took: {:}\".format(training_time))\n    \n    \n    \n    \n    #               Validation\n    # ========================================\n    # After the completion of each training epoch, measure our performance on\n    # our validation set.\n    print(\"\")\n    print(\"Running Validation...\")\n    t0 = time.time()\n    # Put the model in evaluation mode--the dropout layers behave differently\n    # during evaluation.\n    model.eval()\n    # Tracking variables \n    total_eval_accuracy = 0\n    best_eval_accuracy = 0\n    total_eval_loss = 0\n    nb_eval_steps = 0\n    # Evaluate data for one epoch\n    for batch in validation_dataloader:\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        # Tell pytorch not to bother with constructing the compute graph during\n        # the forward pass, since this is only needed for backprop (training).\n        with torch.no_grad():        \n            output= model(b_input_ids, \n                                   token_type_ids=None, \n                                   attention_mask=b_input_mask,\n                                   labels=b_labels)\n        loss = output.loss\n        total_eval_loss += loss.item()\n        # Move logits and labels to CPU if we are using GPU\n        logits = output.logits\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        # Calculate the accuracy for this batch of test sentences, and\n        # accumulate it over all batches.\n        total_eval_accuracy += flat_accuracy(logits, label_ids)\n    # Report the final accuracy for this validation run.\n    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n    # Calculate the average loss over all of the batches.\n    avg_val_loss = total_eval_loss / len(validation_dataloader)\n    # Measure how long the validation run took.\n    validation_time = format_time(time.time() - t0)\n    if avg_val_accuracy > best_eval_accuracy:\n        torch.save(model, 'bert_model')\n        best_eval_accuracy = avg_val_accuracy\n    #print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n    #print(\"  Validation took: {:}\".format(validation_time))\n    # Record all statistics from this epoch.\n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss,\n            'Valid. Accur.': avg_val_accuracy,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    )\nprint(\"\")\nprint(\"Training complete!\")\nprint(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:58:48.151923Z","iopub.execute_input":"2024-10-17T08:58:48.152348Z","iopub.status.idle":"2024-10-17T10:51:12.826251Z","shell.execute_reply.started":"2024-10-17T08:58:48.152313Z","shell.execute_reply":"2024-10-17T10:51:12.825172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the best model ","metadata":{}},{"cell_type":"code","source":"model = torch.load('bert_model')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:51:44.223357Z","iopub.execute_input":"2024-10-17T10:51:44.223741Z","iopub.status.idle":"2024-10-17T10:51:44.507361Z","shell.execute_reply.started":"2024-10-17T10:51:44.223703Z","shell.execute_reply":"2024-10-17T10:51:44.506512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_pre = preprocess_pipe.fit_transform(X_test)\nX_test_pre = X_test_pre.values\nlabels_test = y_test.values ","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:51:53.614083Z","iopub.execute_input":"2024-10-17T10:51:53.614468Z","iopub.status.idle":"2024-10-17T10:55:00.935402Z","shell.execute_reply.started":"2024-10-17T10:51:53.614431Z","shell.execute_reply":"2024-10-17T10:55:00.934372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing","metadata":{}},{"cell_type":"code","source":"test_input_ids = []\ntest_attention_masks = []\nfor review in X_test_pre:\n    encoded_dict = tokenizer.encode_plus(\n                        review,                     \n                        add_special_tokens = True, \n                        max_length = max_len,           \n                        pad_to_max_length = True,\n                        return_attention_mask = True,\n                        return_tensors = 'pt',\n                   )\n    test_input_ids.append(encoded_dict['input_ids'])\n    test_attention_masks.append(encoded_dict['attention_mask'])\n    \ntest_input_ids = torch.cat(test_input_ids, dim=0)\ntest_attention_masks = torch.cat(test_attention_masks, dim=0)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:55:07.394638Z","iopub.execute_input":"2024-10-17T10:55:07.395486Z","iopub.status.idle":"2024-10-17T10:55:51.794088Z","shell.execute_reply.started":"2024-10-17T10:55:07.395444Z","shell.execute_reply":"2024-10-17T10:55:51.793244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TensorDataset(test_input_ids, test_attention_masks)\ntest_dataloader = DataLoader(\n            test_dataset, \n            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n            batch_size = batch_size # Evaluate with this batch size.\n        )","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:57:50.759904Z","iopub.execute_input":"2024-10-17T10:57:50.760646Z","iopub.status.idle":"2024-10-17T10:57:50.765676Z","shell.execute_reply.started":"2024-10-17T10:57:50.760608Z","shell.execute_reply":"2024-10-17T10:57:50.764804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\nfor batch in test_dataloader:\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        with torch.no_grad():        \n            output= model(b_input_ids, \n                                   token_type_ids=None, \n                                   attention_mask=b_input_mask)\n            logits = output.logits\n            logits = logits.detach().cpu().numpy()\n            #pred_flat = np.argmax(logits, axis=1).flatten()\n            \n            predictions.extend(list(logits))","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:57:53.283925Z","iopub.execute_input":"2024-10-17T10:57:53.284787Z","iopub.status.idle":"2024-10-17T11:01:45.856037Z","shell.execute_reply.started":"2024-10-17T10:57:53.284749Z","shell.execute_reply":"2024-10-17T11:01:45.855205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_accuracy = flat_accuracy(predictions, labels_test)\nprint(f'Test accuracy {test_accuracy}') ","metadata":{"execution":{"iopub.status.busy":"2024-10-17T11:02:56.448788Z","iopub.execute_input":"2024-10-17T11:02:56.449193Z","iopub.status.idle":"2024-10-17T11:02:56.468226Z","shell.execute_reply.started":"2024-10-17T11:02:56.449155Z","shell.execute_reply":"2024-10-17T11:02:56.467359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report \n\n\npred_flat = np.argmax(predictions, axis=1).flatten()\nlabels_flat = labels_test.flatten()\nprint(classification_report(labels_flat, pred_flat))","metadata":{"execution":{"iopub.status.busy":"2024-10-17T11:06:11.573136Z","iopub.execute_input":"2024-10-17T11:06:11.573495Z","iopub.status.idle":"2024-10-17T11:06:11.613378Z","shell.execute_reply.started":"2024-10-17T11:06:11.573464Z","shell.execute_reply":"2024-10-17T11:06:11.612343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference ","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\n\nmodel = torch.load('/kaggle/working/bert_model', map_location=torch.device('cuda'))  \nmodel.eval()  \n\n# Load the same tokenizer used during training.\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Function to preprocess text data (tokenize and create attention masks)\ndef preprocess_for_bert(text_list, max_len):\n    \"\"\"\n    Tokenize and create attention masks for input text.\n    Args:\n        text_list (list of str): List of input sentences to tokenize.\n        max_len (int): Maximum sequence length (should match the max_len used during training).\n    Returns:\n        input_ids: Tensor of token IDs.\n        attention_masks: Tensor of attention masks.\n    \"\"\"\n    input_ids = []\n    attention_masks = []\n\n    # Tokenize each sentence in the input list\n    for text in text_list:\n        # Tokenize the input text and add special tokens `[CLS]` and `[SEP]`\n        encoded_dict = tokenizer.encode_plus(\n            text,                      # Sentence to encode\n            add_special_tokens=True,    # Add '[CLS]' and '[SEP]'\n            max_length=max_len,         # Pad or truncate the sentence\n            pad_to_max_length=True,\n            return_attention_mask=True, # Create attention mask\n            return_tensors='pt',        # Return PyTorch tensors\n        )\n        \n        # Append the encoded inputs and the attention masks\n        input_ids.append(encoded_dict['input_ids'])\n        attention_masks.append(encoded_dict['attention_mask'])\n\n    # Convert to tensors\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n\n    return input_ids, attention_masks\n\n# Function to perform inference on new text data\ndef predict(text_list, max_len, device):\n    # Step 1: Apply custom preprocessing (tokenization, stopword removal, etc.)\n    preprocessed_texts = preprocess_pipe.transform(pd.Series(text_list))\n    \n    # Step 2: Convert preprocessed text into BERT input format\n    input_ids, attention_masks = preprocess_for_bert(preprocessed_texts, max_len)\n\n    # Move inputs to the correct device\n    input_ids = input_ids.to(device)\n    attention_masks = attention_masks.to(device)\n\n    # Perform inference without gradient calculation\n    with torch.no_grad():\n        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks)\n\n    # Extract logits and convert to predicted labels\n    logits = outputs.logits\n    predictions = torch.argmax(logits, dim=1).cpu().numpy()\n\n    return predictions\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T11:20:12.307206Z","iopub.execute_input":"2024-10-17T11:20:12.307916Z","iopub.status.idle":"2024-10-17T11:20:12.802937Z","shell.execute_reply.started":"2024-10-17T11:20:12.307873Z","shell.execute_reply":"2024-10-17T11:20:12.801919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example usage\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\npredictions_mapping = {\n    0: \"Positive\",\n    1: \"Negative\"\n}\n\n# Example list of sentences for inference\ntexts = [\n    \"This is a Greate Product, i can't wait to try it \",\n    \"This product is terrible and I am not happy with it.\"\n]\n\n# Define the maximum length for tokenizing (use the same max_len as training)\nmax_len = 200  \n\n# Get predictions for the input sentences\npredictions = predict(texts, max_len, device)\n\nfor i, text in enumerate(texts):\n    print(f\"Text: {text}\")\n    print(f\"Prediction: {predictions_mapping[predictions[i]]}\")  # 0 for negative, 1 for positive \n    print(\"\")","metadata":{"execution":{"iopub.status.busy":"2024-10-17T11:21:21.323089Z","iopub.execute_input":"2024-10-17T11:21:21.323516Z","iopub.status.idle":"2024-10-17T11:21:22.164773Z","shell.execute_reply.started":"2024-10-17T11:21:21.323476Z","shell.execute_reply":"2024-10-17T11:21:22.163809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}