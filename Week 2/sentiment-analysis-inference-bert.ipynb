{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":201678802,"sourceType":"kernelVersion"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook is based on another notebook where we fine-tuned Bert model for sentiment analysis \n\nhere is the link for the notebook [","metadata":{}},{"cell_type":"code","source":"import torch\nimport re\nimport spacy\nimport numpy as np\nimport pandas as pd\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\nfrom tqdm import tqdm\nfrom joblib import Parallel, delayed\n\n# Load Spacy model\nnlp = spacy.load('en_core_web_lg')\n\n# Precompile regex for URL and repeated characters/words\nurl_pattern = re.compile(r'http\\S+|www\\S+|https\\S+')\nrepeated_char_pattern = re.compile(r'(.)\\1{3,}')\nrepeated_word_pattern = re.compile(r'\\b(\\w+)( \\1\\b)+')\n\n# Custom stopwords excluding important negation words\nstopwords_to_keep = {\"not\", \"no\", \"nor\"}\ncustom_stopwords = nlp.Defaults.stop_words - stopwords_to_keep\n\n# Function to remove repeated characters and words\ndef remove_gibberish(text):\n    text = repeated_char_pattern.sub(r'\\1', text)\n    text = repeated_word_pattern.sub(r'\\1', text)\n    return text\n\n# Main text preprocessing function\ndef preprocess_text(text):\n    # Remove URLs\n    text = url_pattern.sub('', text)\n    text = remove_gibberish(text)\n    \n    doc = nlp(text)\n    \n    # Lemmatize, remove stopwords, punctuation, digits, long tokens\n    tokens = [token.lemma_.lower() for token in doc if not (\n        token.is_punct or token.is_digit or len(token.text) > 20 or token.text in custom_stopwords)]\n    \n    # Join tokens back into clean text\n    clean_text = \" \".join(tokens)\n    \n    return clean_text\n\n# Apply preprocessing to a series of texts in parallel\ndef combined_preprocessing(text_series):\n    results = Parallel(n_jobs=-1, backend=\"multiprocessing\")(\n        delayed(preprocess_text)(text) for text in tqdm(text_series, desc=\"Processing texts\")\n    )\n    return pd.Series(results)\n\n# Define a pipeline with custom preprocessing step\npreprocess_pipe = Pipeline([\n    ('preprocess', FunctionTransformer(combined_preprocessing, validate=False)),\n])\n\n# --- BERT Tokenization and Inference ---\n\n# Load pretrained BERT model and tokenizer\nmodel = torch.load('/kaggle/input/preprocessing-pipeline-and-modeling-amazon-reviews/bert_model', map_location=torch.device('cpu'))  # Load saved model\nmodel.eval()  # Set model to evaluation mode\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Function to preprocess text for BERT tokenization\ndef preprocess_for_bert(text_list, max_len):\n    input_ids = []\n    attention_masks = []\n\n    # Tokenize each sentence and create attention masks\n    for text in text_list:\n        encoded_dict = tokenizer.encode_plus(\n            text,                      # Text to encode\n            add_special_tokens=True,    # Add '[CLS]' and '[SEP]'\n            max_length=max_len,         # Pad or truncate to max length\n            pad_to_max_length=True,\n            return_attention_mask=True, # Create attention mask\n            return_tensors='pt',        # Return PyTorch tensors\n        )\n        input_ids.append(encoded_dict['input_ids'])\n        attention_masks.append(encoded_dict['attention_mask'])\n\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n    \n    return input_ids, attention_masks\n\n# Function to perform inference using the BERT model\ndef predict(text_list, max_len, device):\n    # Step 1: Apply custom preprocessing (tokenization, stopword removal, etc.)\n    preprocessed_texts = preprocess_pipe.transform(pd.Series(text_list))\n    \n    # Step 2: Convert preprocessed text into BERT input format\n    input_ids, attention_masks = preprocess_for_bert(preprocessed_texts, max_len)\n\n    # Move inputs to the correct device\n    input_ids = input_ids.to(device)\n    attention_masks = attention_masks.to(device)\n\n    # Perform inference without gradient calculation\n    with torch.no_grad():\n        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks)\n\n    # Extract logits and convert to predicted labels\n    logits = outputs.logits\n    predictions = torch.argmax(logits, dim=1).cpu().numpy()\n\n    return predictions\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-17T11:25:52.424203Z","iopub.execute_input":"2024-10-17T11:25:52.424817Z","iopub.status.idle":"2024-10-17T11:26:00.474328Z","shell.execute_reply.started":"2024-10-17T11:25:52.424761Z","shell.execute_reply":"2024-10-17T11:26:00.473099Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/3787958802.py:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load('/kaggle/input/preprocessing-pipeline-and-modeling-amazon-reviews/bert_model', map_location=torch.device('cpu'))  # Load saved model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bea0c20ed9eb4d179d0aea1ce9c37229"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"891115d9d2144e21804aa86f36e7fbd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f92d6b64c0334d95bbfbed5449798cf3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5098d78c47f4fb48f7e37468dff01ee"}},"metadata":{}}]},{"cell_type":"code","source":"# Example usage\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\npredictions_mapping = {\n    0: \"Positive\",\n    1: \"Negative\"\n}\n\n# Example list of sentences for inference\ntexts = [\n    \"This is a Greate Product, i can't wait to try it \",\n    \"This product is terrible and I am not happy with it.\"\n]\n\n# Define the maximum length for tokenizing (use the same max_len as training)\nmax_len = 200  \n\n# Get predictions for the input sentences\npredictions = predict(texts, max_len, device)\n\nfor i, text in enumerate(texts):\n    print(f\"Text: {text}\")\n    print(f\"Prediction: {predictions_mapping[predictions[i]]}\")  # 0 for negative, 1 for positive \n    print(\"\")","metadata":{"execution":{"iopub.status.busy":"2024-10-17T11:26:43.404786Z","iopub.execute_input":"2024-10-17T11:26:43.405235Z","iopub.status.idle":"2024-10-17T11:26:44.421301Z","shell.execute_reply.started":"2024-10-17T11:26:43.405189Z","shell.execute_reply":"2024-10-17T11:26:44.419891Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Processing texts: 100%|██████████| 2/2 [00:00<00:00,  5.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Text: This is a Greate Product, i can't wait to try it \nPrediction: Positive\n\nText: This product is terrible and I am not happy with it.\nPrediction: Negative\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}